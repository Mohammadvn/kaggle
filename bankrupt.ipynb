{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## IMPORT LIBRARY","metadata":{"id":"fiRYcQsR-Qj7"}},{"cell_type":"markdown","source":"Import the required libraries such as the numpy library, matplotlib, sklearn and others.","metadata":{"id":"z2PHoCGsWCaO"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport joblib\nimport keras\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, classification_report, jaccard_score, log_loss, mean_squared_error, confusion_matrix\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import cross_val_score\nfrom keras.utils import np_utils\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.regularizers import l2\nfrom keras.optimizers import SGD\nfrom imblearn.over_sampling import SMOTE","metadata":{"id":"akIv0ULy3IbM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IMPORT DATASET","metadata":{"id":"96c3aqOA-U2Y"}},{"cell_type":"markdown","source":"Calling the dataset in csv format and stored in the \"df_train\" variable then displaying the top 5 data using the .head() syntax\n\n","metadata":{"id":"o-XitZ0lWZIW"}},{"cell_type":"code","source":"df = pd.read_csv(\"/content/drive/MyDrive/Kaggle/company_bankcruptcy.csv\")\ndf.head()","metadata":{"id":"xKSAyRDC3VFV","outputId":"64d32e57-a728-444d-e763-7061c89a555a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"Re1628p9bsL7","outputId":"42c443bd-008b-4f21-f929-10ab82018afd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{"id":"_WndCCTV-bka"}},{"cell_type":"markdown","source":"In the early stages of EDA, the first thing to do is look at the information from the dataset using the info() syntax. By using this syntax we can see the amount of data in each column and the data type. Because this dataset aims to predict whether the company will go bankrupt or not, the data needed is data in int and float format. from this dataset it can also be seen that there are 95 features and 1 target. After that other information can be seen using the syntax describe(). With this syntax we can see the average value and standard deviation of the dataset.","metadata":{"id":"j3auiogwXcX1"}},{"cell_type":"code","source":"df.info()\ndf.describe()","metadata":{"id":"KFZ2bK8h3ZmE","outputId":"2d5171ea-6dd6-44d8-dd0e-f1be8b11a0dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look at the dataset whether there are null and duplicate data","metadata":{"id":"UaV7XtB8-xut"}},{"cell_type":"code","source":"print(\"=========== null of dataset================== \")\nprint(df.isnull().values.any())\nprint(\"=========== Sum Duplicate of dataset================== \")\ndf[df.duplicated()]","metadata":{"id":"O5dp7D2m0bU0","outputId":"bb29426e-6edc-42d8-860d-60cc75c76710"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because this dataset is included in the classification, balancing the target data must be considered. How to see it can use the value_counts() syntax to find out the amount of data in each column.","metadata":{"id":"3do7YI9b_HEI"}},{"cell_type":"code","source":"df['Bankrupt?'].value_counts()","metadata":{"id":"3JG2Dw8j1a-u","outputId":"68c75a5b-d28f-4df5-c75d-91d4966fb9ff"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing Data","metadata":{"id":"F42gcMAb_K1R"}},{"cell_type":"markdown","source":"The first stage is to overcome the problem of data inbalancing. From the EDA process we can find out the amount of data in class 0 in the target column there are 6599 data and class 0 is 220 data. With this amount of data I try to use SMOTE because if you use Undersampling then around 6300 data will be wasted.","metadata":{"id":"Ok3fvSvx_OfI"}},{"cell_type":"code","source":"X = df.drop('Bankrupt?', axis=1).reset_index(drop=True)\ny = df['Bankrupt?'].reset_index(drop=True)","metadata":{"id":"Aan-8ex4L2Xh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SMOTE on Datasets\nsm = SMOTE(random_state = 2)\nX_smote, y_smote = sm.fit_resample(X, y.ravel())\n\n# Give back value after SMOTE to df\ndf = X_smote\ndf['Bankrupt'] = y_smote","metadata":{"id":"zlYPrdNhMQH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Bankrupt'].value_counts()","metadata":{"id":"VIrKNcMFN2ft","outputId":"dcccd5b8-96fd-4fec-c245-a56d4a033ca4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then I try to check again whether there are null or duplicate data. This stage is optional and can be done again or not.","metadata":{"id":"i4MoFQoCEM_f"}},{"cell_type":"code","source":"print(\"=========== Sum null of dataset================== \")\nprint(df.isnull().values.any())\nprint(\"=========== Sum Duplicate of dataset================== \")\ndf[df.duplicated()]","metadata":{"id":"BpgN3oIm5WGk","outputId":"dd611276-5d50-407a-8978-38b2ebf2ae39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that look at the correlation between the data. In the image below, what must be seen is the correlation between the Bankrupt data and other data. If you look at the picture, the highest correlation is found in Borrowing Dependency.\n\n","metadata":{"id":"vsS2w7LaEi4s"}},{"cell_type":"code","source":"corr = df.corr()[['Bankrupt']].sort_values(by='Bankrupt', ascending=False)\nsns.heatmap(corr, annot=True)","metadata":{"id":"qbOuWAOr34fP","outputId":"92501492-f49b-4e0a-b225-2928cf491521"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then look at the distribution of data in the dataset using boxplot plots. By using a boxplot you can also check whether the data has outliers or not. If there is then the outlier must be removed.\n\n","metadata":{"id":"M5yHPuxsZoL6"}},{"cell_type":"code","source":"for column in df:\n    plt.figure()\n    df.boxplot([column])","metadata":{"id":"arWNBbNpbv2x","outputId":"f4968c4a-a611-4523-cc50-5dcef25c3e07"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following is the syntax for removing outliers in the dataset.","metadata":{"id":"WsFYQ7nHFhfp"}},{"cell_type":"code","source":"for i in df.columns:\n    Q1 = df[i].quantile(0.25)\n    Q3 = df[i].quantile(0.75)\n    IQR = Q3 - Q1\n    df[i] = np.where(df[i]>(Q3+1.5*IQR),(Q3+1.5*IQR),df[i])\n    df[i] = np.where(df[i]<(Q1-1.5*IQR),(Q1-1.5*IQR),df[i])","metadata":{"id":"IMJGoId2N_AH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check again whether the outlier data has been deleted or not.","metadata":{"id":"lRYA7hOnFo_K"}},{"cell_type":"code","source":"# for column in df:\n#     plt.figure()\n#     df.boxplot([column])","metadata":{"id":"ph5pXZNYNwkX","outputId":"9c9d0dff-678f-4043-b6d4-a506e3929e23"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, also look at the distribution of data using histogram plotting.","metadata":{"id":"t524Ux0DFuA7"}},{"cell_type":"code","source":"df.hist(bins=50, figsize=(20,15))\nplt.show()","metadata":{"id":"iOXN6WcHU77V","outputId":"d0834101-0277-4fff-8e13-152ea57c1d06"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the information from the dataset is sufficient, the next step is to separate the data between feature data and target data. The reason is because the next stage is data scaling and data scaling is only done on feature data.","metadata":{"id":"HQ-EgsflFyi_"}},{"cell_type":"code","source":"X = df.drop('Bankrupt', axis=1).reset_index(drop=True)\ny = df['Bankrupt'].reset_index(drop=True)","metadata":{"id":"qaMt6L9aQfGl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this stage, scaling the data using a standard scaler. The purpose of the standard scaler is to make the mean value 0 and the variance 1.","metadata":{"id":"dmpbB2-6dmhH"}},{"cell_type":"code","source":"tf = StandardScaler().fit_transform(X)\nscaledf = pd.DataFrame(tf, columns=X.columns)\nscaledf['Bankrupt'] = y\nscaledf","metadata":{"id":"M69S8DC_Qn35","outputId":"77a7195e-53cb-4eed-9745-e3a2029fae62"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checks whether the mean value on the df scale is close to 0 and the standard deviation is 1","metadata":{"id":"p0BoR2dvBwXl"}},{"cell_type":"code","source":"print(scaledf.isnull().values.any())\nscaledf.describe()","metadata":{"id":"PnAVvp8L5va0","outputId":"d6c2cd10-b1b2-4a01-bbe9-c6029fa4cd82"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then the data that has been scaled will be divided into 4 parts, namely x_test, x_train, y_train, and y_test. The division of the data is 80% train data and 20% test data.\n","metadata":{"id":"lF0Z4EG4f9zp"}},{"cell_type":"code","source":"xx = scaledf.drop('Bankrupt', axis=1)\nyy = scaledf['Bankrupt']\nX_train, X_test, y_train, y_test = train_test_split(xx, yy, test_size=0.2, random_state=42)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","metadata":{"id":"BLOFBD-IO84z","outputId":"8ba6e432-f509-4ddb-dd81-180c169a382d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a validation variable by means of reverse data.","metadata":{"id":"k-averIANUgn"}},{"cell_type":"code","source":"x_val = X_train[-2640:]\ny_val = y_train[-2640:]","metadata":{"id":"54rZvPyZ31qa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{"id":"xZCFTLH9B_u4"}},{"cell_type":"markdown","source":"Create a model with 3 layers, namely 1 input layer, 1 hidden layer, and 1 output layer. The input layer uses 95 nodes and requires relative activation. 95 nodes were chosen because in the input layer the number of nodes is the number of features used and because the data that has been scaled has negative value data, relu activation is used. After that, the hidden layer uses 2 nodes and activates Relu. Then the output layer uses 1 number of nodes and the sigmoid activation function. The number of these nodes depends on the type of classification contained in the dataset, because the dataset has 2 classes in the target column, this classification is called binary classification. And Binary classification uses the sigmoid activation function for the output layer with the number of nodes 1.","metadata":{"id":"pGb4OLPg0ytC"}},{"cell_type":"code","source":"model = keras.Sequential()\nmodel.add(Dense(units = 95, activation='relu', input_dim= 95)) #input layer\nmodel.add(Dense(units = 2, activation='relu')) #hidden layer\nmodel.add(Dense(units = 1, activation='sigmoid')) # output layer","metadata":{"id":"JngVnz49b-XW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is a summery model that has been made","metadata":{"id":"Fd9hPv4KPIGv"}},{"cell_type":"code","source":"model.summary()","metadata":{"id":"gnTBDgdadPLQ","outputId":"32d72456-6f0c-45c6-edf4-bb8b56af87ce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next stage is compiling the model. The optimizer used is ADAM with a learning rate of 0.0001. Then the loss used is Binary crossentropy, this loss was chosen because for the binary class loss classification case used is Binary crossentropy. And for the metrics use accuracy.","metadata":{"id":"JxQQefeCPMAx"}},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                  metrics=['accuracy'])","metadata":{"id":"p1hiakEKct7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then the model will be fitted with batch size 16 and epoch 30. And obtained loss 0..03, accuracy 0.99, val loss 0.02, and val accuracy 0.99\n\n","metadata":{"id":"dn2CCnQEQGl0"}},{"cell_type":"code","source":"history = model.fit(\n    X_train,\n    y_train,\n    batch_size=16,\n    epochs=30,\n    validation_data=(x_val, y_val),\n)\n","metadata":{"id":"ioWu6OML3_ro","outputId":"a1a44863-d51f-4f79-e4b9-43339ca4b8ea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{"id":"s3_eNf4vDeT5"}},{"cell_type":"markdown","source":"Plotting the loss and accuracy of the model that has been made.","metadata":{"id":"78jfrmSeUK8x"}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(10,4))\nplt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","metadata":{"id":"OKKMz1jfTOX5","outputId":"4fd034a9-dee3-47e9-8ee2-f729f79fc744"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"syntax to get prediction results using np.round","metadata":{"id":"iaD50Ku5DjxQ"}},{"cell_type":"code","source":"kelas = np.round(model.predict(X_test),0)\nhasil_prediksi = np.asarray(kelas, dtype = 'int')\nprint(hasil_prediksi)\nypred = hasil_prediksi","metadata":{"id":"k0roosQtPuiz","outputId":"444473b3-0423-4dda-c281-7ed76bd104aa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following is the confusion matrix of the model","metadata":{"id":"AkwHCMUnDsO_"}},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_true=y_test, y_pred=ypred)\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\nax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(conf_matrix.shape[0]):\n  for j in range(conf_matrix.shape[1]):\n    ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()\nreport = classification_report(ypred, y_test)\nprint(report)","metadata":{"id":"GC2AMItFPu-i","outputId":"b33eb05e-65b9-41df-f966-db30cfe3ec09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"KUBmYKQl6fxP"},"execution_count":null,"outputs":[]}]}